\chapter{Some Special Functions}
Power series, $e^{x}, \log{x},\sin{x},\cos{x}$, Fourier series (we're omitting Gamma function)

\section{Power Series}
Recall $\sum_{n=0}^{\infty}{c_n{x^{n}}}$ has a radius of convergence $R=\frac{1}{\limsup_{n\to \infty}{\sqrt[n]{\left|c_{n}\right| }}}$ such that the series absolutely converges for $\left|x\right| < R$, diverges for $\left|x\right| >R$ and anything possible for $\left|x\right| =R$.

\begin{remark}
	To determine $R$, we often use the ratio test instead:
	\[
		\left|\frac{c_{n+1}x^{n+1}}{c_nx^{n}}\right| =\left|x\right| \cdot \left|\frac{c_{n+1}}{c_n}\right|\underbrace{\to}_{\text{ if limit exists } } \left|x\right| \cdot L\implies \text{ absolute convergence if } \left|x\right| <\frac{1}{L}, \text{ diverges if }  \left|x\right| >\frac{1}{L}
		,\] where $R=\frac{1}{L}=\frac{1}{\lim_{n\to \infty}{\left|\frac{c_{n+1}}{c_n}\right| }}$.\\
	In general, by Theorem~3.37, we also have \[
		\frac{1}{\liminf_{n\to \infty}{\left|\sqrt[n]{c_n}\right|}}\le R\le \frac{1}{\liminf_{n\to \infty}{\frac{c_{n+1}}{c_n}}}
		.\]
\end{remark}

\begin{theorem}[1]
	Suppose $\sum_{n=0}^{\infty}{c_n x^{n}}$ has a radius of convergence $R>0$ and define $f(x)=\sum_{n=0}^{\infty}{c_n x^{n}}$ for $\left|x\right| < R$. Such a function $f(x)$ is called an \textit{analytic function} .\\
	If $R<\infty$, then the series converges uniformly on $[-R+\epsilon,R-\epsilon]$ for all $\epsilon>0$.\\
	If $R=\infty$, then the series converges uniformly on $[-M,M]$ for all $M < \infty$.
	The function $f$ is continuous and differentiable on $(-R,R)$ with $f'(x)=\sum_{n}{c_n n x^{n-1}}$
	\begin{note}
		a function of the form $f(x)=\sum_{n=0}^{\infty}{c_n x^{n}}$ is called an \textit{analytic function} .\\
	\end{note}
	\begin{remark}
		Uniform convergence may not hold on $(-R,R)$. C.f. A7-Q3.
	\end{remark}
	\begin{proof}
		\hfill
		\begin{description}
			\item[Uniform convergence:]
			      For $\left|x\right| \le R-\epsilon$, $\left|c_n x^{n}\right|\le \left|c_n\right| (R-\epsilon)^{n}$.\\
			      $\sum_{n=0}^{\infty}{\left|c_n\right|(R-\epsilon)^{n}}<\infty$ by absolute convergence on $(-R,R)$. Hence, by \ref{thm:M-test}, $\sum_{n=0}^{\infty}{c_n x^{n}}$ converges uniformly on $[-R+\epsilon,R-\epsilon]$.\\
			\item[Derivative:]
			      The radius of convergence of $\sum_{n=1}^{\infty}{nc_n x^{n-1}}$ is
			      \begin{flalign*}
				      \frac{1}{\limsup_{n\to \infty}{\sqrt[n]{n \left|c_n\right| }}} & =\frac{1}{\limsup_{n\to \infty}{\sqrt[n]{n}\sqrt[n]{\left|c_n\right|}}}                                                      \\
				                                                                     & =R =\text{ radius of convergence of } \sum_{n=0}^{\infty}{c_n x^{n}} \text{  ($\because \lim_{n\to \infty}{\sqrt[n]{n}}=1$)}
				      .\end{flalign*}
			      Let $S_n(x)=\sum_{m=0}^{n}{c_m x^{m}}$. Then $S'_n(x)=\sum_{m=1}^{n}{c_m m x^{m-1}}$.
			      By the first part of the proof, $S'_n(x)\to \sum_{m=1}^{\infty}{c_m m x^{m-1}}$ uniformly on $[-R+\epsilon, R-\epsilon]$. Since also $S_n(x)\to f(x)$, by Theorem~\ref{thm:7.17}, $f'$ exists on $[-R+\epsilon,R-\epsilon]$, and $f'(x)=\sum_{m=1}^{\infty}{c_m m x^{m-1}}$.\\
			      Since $\epsilon$ is arbitrary, $f'(x)=\sum_{m=1}^{\infty}{c_m m x^{m-1}}$ for all $x \in (-R,R)$. In particular, $f$ is also continuous.
		\end{description}
	\end{proof}
\end{theorem}

\begin{corollary}
	If $f(x)=\sum_{n=0}^{\infty}{c_n x^{n}}$ converges for $\left|x\right| < R$, then $f^{(k)}(x)$ exists for all $k \in \N$, and
	\begin{equation*}
		f^{(k)}(x)=\sum_{n=k}^{\infty}{c_n n(n-1)\cdots (n-k+1)x^{n-k}}
		\tag{*}
		.\end{equation*}
	Consequently, $c_k=f^{(k)}(0)$ and $f(x)=\sum_{n=0}^{\infty}{\frac{f^{(n)}(0)}{n!} x^{n}}$.
	\begin{note}
		C.f. Taylor's theorem.
	\end{note}
	\begin{proof}
		By Theorem~\ref{thm:8.1}, $f'(x)=\sum_{n=1}^{\infty}{c_n n x^{n-1}}, f''(x)=(f')'(x)=\sum_{n=2}^{\infty}{c_n n(n-1)x^{n-2}}, \ldots $.\\
		Set $x=0$ in (*) to get $f^{(k)}(0)\underbrace{=}_{\text{ only } n=k \text{ term survives } }c_k k (k-1) \cdots \cdot 1=c_k\cdot  k!$.
	\end{proof}
\end{corollary}
\begin{example}
	Let $f(x)=\begin{cases}
			e^{-\frac{1}{x^2}} & x\neq 0 \\
			0                  & x=0
		\end{cases}$.
	By Rudin's problem 8.1, $f^{(n)}(0)=0$ for all $n=0,1,2,\ldots $, so
	$f(x)\neq \sum_{n=0}^{\infty}{\frac{f^{(n)}}{n!} x^{n}}$ except for $x=0$.
	\begin{remark}[Bump Function]
		\textit{Bump functions} are infinitely differentiable functions with compact support. E.g.,
		\[
			f(x)=\begin{cases}
				e^{-\frac{1}{1-x^2}} & x \in (-1,1)         \\
				0                    & \left|x\right| \ge 1
			\end{cases}
			.\]
	\end{remark}
\end{example}

\begin{thm}[2][\namedlabel{thm:abel}{Abel's Theorem}]
	Suppose $\sum_{n=0}^{\infty}{c_n}$ converges (perhaps conditionally). Let $f(x)=\sum_{n=0}^{\infty}{c_n x^{n}}$.
	Then $f(x)$ converges for $\left|x\right| < 1$ and $\lim_{x\to 1^{-}}{f(x)}=f(1)=\sum_{n=0}^{\infty}{c_n}$.
	\begin{remark}
		Interesting case is at $R=1$, since $R>1$ implies continuity of $f$ for $\left|x\right| < R$.
	\end{remark}
	\begin{proof}
		By the root test, $\limsup_{n\to \infty}{\sqrt[n]{\left|c_n\right|}}\le 1$, so $\sum_{n=0}^{\infty}{c_n x^{n}}$ has $R\ge 1$.
		Let $S_n=\sum_{m=0}^{n}{c_m}$ and $S=\sum_{m=0}^{\infty}{c_m}=\lim_{n\to \infty}{S_n}$.\\
		Set $S_{-1}=0$. Then $c_{n}=s_{n}-s_{n-1}$ for $n\ge 0$.\\
		Let $\epsilon>0$. We need to show $\exists{\delta > 0} \text{ such that } 1-\delta<x<1 \implies \left|f(x)-S\right| < \epsilon$.\\
		Start with partial sum for $f(x)$. For $\left|x\right| < 1$,
		\begin{flalign*}
			\sum_{m=0}^{n}{c_m x^{m}}=\sum_{m=0}^{n}{(S_m-S_{m-1})x^{m}}=\sum_{m=0}^{n}{S_m x^{m}}-\sum_{m=0}^{n}{S_{m-1}x^{m}}
			.\end{flalign*}
		Let $k=m-1$ so that $m=k+1$. Then
		\begin{flalign*}
			 & \sum_{m=0}^{n}{S_{m-1}x^{m}} =\underbrace{S_{0-1}}_{0}\cdot 1+\sum_{m=1}^{n}{S_{m-1}x^{m}} =x\sum_{k=0}^{n-1}{S_k x^{k}}                                                        \\
			 & \sum_{m=0}^{n}{c_m x^{m}}     =(1-x)\sum_{m=0}^{n}{S_m x^{m}}+\underbrace{S_n x^{n+1}}_{\to 0 \text{ as } n\to \infty \text{ since $S_n$ is bounded and $\left|x\right| < 1$} }
			.\end{flalign*}
		Let $n\to \infty$.
		Then
		\begin{flalign*}
			f(x) & =(1-x)\sum_{n=0}^{\infty}{S_n x^{n}}
			.\end{flalign*}
		\begin{flalign*}
			\left|f(x)-S\right| & =\left|(1-x)\sum_{n=0}^{\infty}{S_n x^{n}}-S(1-x) \cdot \frac{1}{1-x}\right|
			=	\left|(1-x)\sum_{n=0}^{\infty}{S_n x^{n}}-S(1-x)\sum_{n=0}^{\infty}{x^{n}}\right|                \\                                                                                   & =(1-x) \left|\sum_{n=0}^{\infty}{(S_n-S)x^{n}}\right|\le (1-x)\sum_{n=0}^{\infty}{\left|S_n-S\right|\cdot \left|x \right|^{n}  }
			.\end{flalign*}
		Choose $N$ s.t. $n\ge N \implies \left|S_n-S\right|<\frac{\epsilon}{2} $.
		For $x \in (0,1)$,
		\begin{flalign*}
			\left|f(x)-S\right| & \le (1-x) \sum_{n=0}^{N}{\left|S_n-S\right| x^{n}}+(1-x) \sum_{n=N+1}^{\infty}{\left|S_n-S\right|x^{n}}     \\
			                    & <(1-x) \sum_{n=0}^{N}{\left|S_n-S\right| x^{n}} +(1-x)\left( \frac{\epsilon}{2} \cdot \frac{1}{1-x}\right)=
			(1-x) \left(\sum_{n=0}^{N}{\left|S_n-S\right| x^{n}}\right)+\frac{\epsilon}{2}
			.\end{flalign*}
		Since $(1-x)\sum_{n=0}^{N}{\left|S_n-S\right| x^{n}}$ is a polynomial in $x$, so it is continuous and equals $0$ at $x=1$.\\
		Hence, $(1-x)\sum_{n=0}^{N}{\left|S_n-S\right| x^{n}}<\frac{\epsilon}{2}$ if $\left|x-1\right| <\delta$ for some $\delta>0$. Therefore, for $1-\delta<x<1$, $\left|f(x)-S\right|<\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon$.
	\end{proof}
	\begin{note}
		For an application of Abel's theorem, see Rudin's p. 175.\\
		For the case $\sum_{n=0}^{\infty}{c_n}=\infty$, see A7.
	\end{note}
\end{thm}

\begin{thm}[3]
	If $\sum_{i=1}^{\infty}{\sum_{j=1}^{\infty}{\left|a_{ij}\right|}}$ converges, then \[
		\sum_{i=1}^{\infty}{\sum_{j=1}^{\infty}{a_{ij}}}= \sum_{j=1}^{\infty}{\sum_{i=1}^{\infty}{a_{ij}}},\] where both sides converge.
	\begin{proof}
		Rudin has a too clever proof \ldots  A7 involves a more straightforward proof.
	\end{proof}
\end{thm}

\begin{thm}[4]
	Suppose $f(x)=\sum_{n=0}^{\infty}{c_n x^{n}}$ (Taylor series of $f$ at $x=0$, a.k.a Maclauren series) has a radius of convergence $R>0$.
	Let $\left|a\right|<R$.
	Then $f(x)=\sum_{n=0}^{\infty}{\frac{f^{(n)}(a)}{n!}(x-a)^{n}}$ for (at least) $\left|x-a\right|<R-\left|a\right|$.
	\begin{proof}
		Note \[
			f(x)=\sum_{n=0}^{\infty}{c_n \left( a+(x-a) \right)^{n}}=\sum_{n=0}^{\infty}{c_n}\sum_{m=0}^{n}{\binom{n}{m}(x-a)^{m}a^{n-m}}
			.\]
		We want to interchange the order of summation.\\
		By Theorem~\ref{thm:8.3}, interchange of summations is justified if \[
			\sum_{n=0}^{\infty}{\sum_{m=0}^{n}{\left|c_n\right|\binom{n}{m}\left|x-a\right|^{m}\left|a\right|^{n-m}}}<\infty
			.\]
		Note $\sum_{n=0}^{\infty}{\left|c_{n}\right|\left( \left|x-a\right|+\left|a\right|\right)^{n}}$ does converge as we assume $\left|x-a\right|+\left|a\right|<R$.\\
		Therefore, for $\left|x-a\right| < R-\left|a\right|$,
		\begin{flalign*}
			f(x) & =\sum_{m}^{\infty}\frac{1}{m!}(x-a)^{m}{\sum_{n=m}^{\infty}{c_n[ n\cdot (n-1)(n-2)\cdots \cdot (n-m+1)] a^{n-m}}} \\
			     & =\sum_{m}{\left(\sum_{n}{c_n \binom{n}{m}a^{n-m}}\right)(x-a)^{m}}                                                \\
			     & =\sum_{m}{\left(\sum_{n}{\frac{f^{(n)}(a)}{n!}n(n-1)\cdots (n-m+1)a^{n-m}}\right)(x-a)^{m}}                       \\
			.\end{flalign*}
	\end{proof}
	\begin{example}
		Let $f(x)=\sum_{n=0}^{\infty}{x^{n}}$ for $\left|x\right|<1 $. Then $f(x)=\frac{1}{1-x}$ for $\left|x\right|<1$.\\
		Taylor series of $\frac{1}{1-x}$ at $x=-\frac{1}{2}$:
		For $\left|x\right| < 1$, $f^{(n)}(x)=\frac{n!}{(1-x)^{n+1}}$, so $f^{(n)}(-\frac{1}{2})=\frac{n!}{(\frac{3}{2})^{n+1}}$.\\
		By Theorem~\ref{thm:8.4},
		\[
			f(x)=\sum_{n=0}^{\infty}{\frac{n!}{(\frac{3}{2})^{n+1}n!}\left(x+\frac{1}{2}\right)^{n}}=\sum_{n=0}^{\infty}{(\frac{2}{3})^{n}\left(x+\frac{1}{2}\right)^{n}}
		\]
		for $\left|x+\frac{1}{2}\right|<1-\left|-\frac{1}{2}\right| =\frac{1}{2}$.\\
		In fact, the series converges even when $\left|\frac{2}{3}\left(x+\frac{1}{2}\right)\right|<1$; i.e., $\left|x+\frac{1}{2}\right| <\frac{3}{2}$. C.f. Analytic continuation.\\
		Another way to get Taylor series at $x=-\frac{1}{2}$:\\
		For $\left|x\right|<1 $,
		\[
			f(x)=\frac{1}{1-x}=\frac{1}{\left(1+\frac{1}{2}\right)-\left(x+\frac{1}{2}\right)}=\frac{2}{3} \frac{1}{1-\frac{2}{3}\left(x+\frac{1}{2}\right)}=\frac{2}{3}\sum_{n=0}^{\infty}{\left(\frac{2}{3}\right)^{n}\left(x+\frac{1}{2}\right)^n}.
		\]
	\end{example}
\end{thm}

\begin{thm}[5][\namedlabel{thm:pop}{Principle of Permanence of Form}]
	Suppose $\sum_{n}{a_{n}x^{n}}$ and $\sum_{n}{b_{n}x^{n}}$ have radii of convergence at least  $R$. Suppose $D \subset (-R,R)$ has a limit point in $(-R,R)$.\\
	If $\sum_{n}{a_{n}x^{n}}=\sum_{n}{b_{n}x^{n}}$ for all $x \in D$, then $a_{n}=b_{n}$ for all $n$, and hence $\sum_{n}{a_{n}x^{n}}=\sum_{n}{b_{n}x^{n}}$ for all $\left|x\right|<R$.
	\begin{proof}
		Let $c_{n}=a_{n}-b_{n}$ and $f(x)=\sum_{n=0}^{\infty}{c_n x^{n}}$.
		Then $f(x)=0$ for all $x \in D$.\\
		Let $E=\{x \in (-R,R): f(x)=0\}$. Then $D \subset E$.\\
		We want to show $E=(-R,R)$.
		Let $A=E' \cap (-R,R)$. Then $A\neq \emptyset$ because $D$ has a limit point in $(-R,R)$.\\
		Also, relative to $(-R,R)$, $A$ is closed as the set of all limit points is always closed (C.f. Problem 2.6).\\
		Let $B=(-R,R)\setminus A$. Then $A\cup B=(-R,R)$, $A\cap B=\emptyset$, and $B$ is open.\\
		\begin{claim}
			$A$ is also open.
			\begin{proof}
				Let $x_{0} \in A=E' \cap (-R,R)$.
				Then $\exists{\{ {d}_{n}\}} \text{ such that } f(x)=\sum_{n=0}^{\infty}{d_n (x-x_{0})}^{n}$ for $\left|x-x_{0}\right|<R-\left|x_{0}\right|$.
				If we show that $d_{n}=0$ for all $n\ge 0$, it proves $(x_{0}-r,x_{0}+r) \subset A$, and hence that $A$ is open.
				Suppose for contradiction there exists $k\ge 0$ such that $d_k\neq 0$ and $f(x)=\sum_{n=k}^{\infty}{d_n (x-x_{0})^{n}}$.
				Then $f(x)=(x-x_{0})^{k} \sum_{n=k}^{\infty}{d_n(x-x_{0})^{n-k}}$.
				Let $m=n-k$. Then \[
					f(x)=(x-x_{0})^{k} \sum_{m=0}^{\infty}{d_{m+k}(x-x_{0})^{m}}
					.\]
				Say $g(x)=\sum_{m=0}^{\infty}{d_{m+k}(x-x_{0})^{m}}$. Then
				$g(x_0)=d_k\neq 0$, and by continuity of $g$, $\exists{\delta>0} \text{ such that } g(x)\neq 0$ if $\left|x-x_{0}\right|<\delta$.
				However, $f(x)=(x-x_{0})^{k}g(x)\neq 0	 \text{ if } 0<\left|x-x_{0}\right|<\delta$, so $x_{0}$ is an isolated zero of $f$ and $x_{0} \in E'$.
				This is a contradiction, so $d_{n}=0$ for all $n\ge 0$.
			\end{proof}
		\end{claim}
		Given the claim, in $(-R,R)$, $A$ and $B$ are both open and closed.
		Since $(-R,R)$ is connected (C.f. MATH-320's A5.3(c)),
		one of \begin{enumerate}[label=(\roman*)]
			\item $A=\emptyset,B=(-R,R)$
			\item $A=(-R,R),B=\emptyset$
		\end{enumerate}
		must hold.\\
		As $A$ is non-empty, $A=(-R,R)=E'$.\\
		Therefore, $\forall{x \in (-R,R)}: \exists{\{ {x}_{n}\} \text{ in } E} \text{ such that } x_n\to x$.
		Since $f$ is continuous on $(-R,R)$, $f(x)=\lim_{n\to \infty}{f(x_n)}=0$.\\
		Therefore, for all $x \in (-R,R)$, $x \in E$, so $(-R,R)\subset E$.\\
	\end{proof}
\end{thm}

\begin{remark}
	CUT-OFF for MT2 ends here, including A4,5,6,7 and up to Rudin's p.143-178.
\end{remark}



\section{Exponential Function}
Recall \[
	e:=\sum_{n=0}^{\infty}{\frac{1}{n!}}
	.\]

Also, $e=\lim_{n\to \infty}{\left(1+\frac{1}{n}\right)^{n}}$, $e <3, e \in \Q$.
\begin{define}
	For $z \in \C$, $E(z):=\sum_{n=0}^{\infty}{\frac{z^{n}}{n!}}$ and it has a radius of convergence $R=\infty$.
	Note $E(1)=e$ is given by the definition of $e$.
\end{define}

\begin{thm}
	For $p \in \Q$, $E(p)=e^{p}$.
	\begin{proof}
		\begin{enumerate}[label=(\roman*)]
			\item
			      Note \begin{flalign*}
				      E(w)E(z) & =\sum_{m=0}^{\infty}{\frac{w^{m}}{m!}} \sum_{n=0}^{\infty}{\frac{z^{n}}{n!}}= \sum_{n=0}^{\infty}{\sum_{k=0}^{n}{\frac{z^{k}}{k!} \frac{w^{n-k}}{(n-k)!}}} \;\; (\text{ by Theorem~\ref{thm:3.50}}) \\
				               & =\sum_{n=0}^{\infty}{\frac{1}{n!} \sum_{k=0}^{n}{\binom{n}{k}z^{k}w^{n-k}}}=\sum_{n=0}^{\infty}{\frac{(z+w)^{n}}{n!}}=E(z+w).
			      \end{flalign*}
			      In particular,
			      \begin{flalign*}
				      E(2) & =E(1+1)=E(1)^{2}=e^2                                        \\
				      E(3) & =E(1+2)=E(1)E(2)=e\cdot e^2=e^3                             \\
				      \vdots
				      E(n) & =e^n \text{ for } n =0, 1,2,3,\ldots, (E(0)=\frac{1}{0!}=1)
				      .\end{flalign*}
			\item  Since $E(z)E^{-z}=E(z-z)=1$, $E(-z)=\frac{1}{E(z)}$.
			      Hence, $E(-n)=\frac{1}{E(n)}=\frac{1}{e^n}=e^{-n}$.\\
			      Thus, $E(n)=e^{n}$ for $n \in \Z$.
			\item Let $p=\frac{m}{n}$, $m \in \Z, n \in \N$. Then
			      \begin{flalign*}
				      e^{m}=E(m)=E(np)=E(p+p+\cdots +p)=E(p)^{n}
				      .\end{flalign*}
			      Hence, $E(\frac{m}{n})=E(p)=e^{\frac{m}{n}}=e^{p}$.
		\end{enumerate}
	\end{proof}
\end{thm}

\begin{define}
	For $x \in \R$, let $e^{x}=\sum_{n=0}^{\infty}{\frac{x^{n}}{n!}}=E(x)$.
	This defines $x \mapsto e^{x}$ as an analytic function on $\R$, which must be infinitely differentiable and \[
		\frac{\mathrm{d}}{\mathrm{d}x}e^{x}=\sum_{n=1}^{\infty}{\frac{1}{n!}n x^{n-1}}= \sum_{n=1}^{\infty}{\frac{1}{(n-1)!} x^{n-1}}=e^{x}
		.\]
	We then have $\frac{\mathrm{d}^{k}}{\mathrm{d}x^{k}}e^{x}=e^{x}$ for all $x=0,1,2,\ldots $.\\
	Since $e^{x}e^{-x}=1$ and since $e^{x}>0$ for $x\ge 0$, $e^{-x}>0$, so $e^{x}>0$ for all $x \in \R$.\\
	From the first two derivatives, $x \mapsto e^{x}$ is strictly increasing and strictly convex.\\
	For $n\ge 0$ and $x>0$, $e^{x}> \frac{x^{n+1}}{(n+1)!}$, so $\frac{e^{x}}{x^{n}}>\frac{x}{(n+1)!}\to \infty$ as $x\to \infty$. \\
	In particular, with $n=0$, $e^{x}\to \infty$ as $x\to \infty$, and $e^{-x}=\frac{1}{e^{x}}\to 0$ as $x\to \infty$.
\end{define}

\begin{define}
	For $y>0$, define $L(y)=\log{y}(= \ln{y})$ by $E(L(y))=y$.
	Equivalently, $L(E(x))=x$ for all $x \in \R$, or
	$e^{\log{y}}=y$ for all $y>0$ and $\log{e^{x}}=x$.\\
	Since $x \mapsto e^{x}$ is strictly increasing and differentiable,
	its inverse $y \mapsto \log{y}$ is also strictly increasing and differentiable.\\
\end{define}
\begin{thm*}
	$L'(y)=\frac{1}{y}$ for $y>0$, $L(uv)=L(U)+L(v)$ for $u,v>0$.
	\begin{proof}
		Apply the chain rule to $L(E(x))=x$ (C.f. Rudin's Problem 5.2).\\
		Then $L'(E(x))E'(x)=1$, so $L'(y)=L'(E(x))=\frac{1}{E(x)}=\frac{1}{y}$.\\
		Also, since there exist unique $x,y$ such that $E(x)=u,E(y)=v$, $L(uv)=L(E(x)E(y))=L(E(x+y))=x+y=L(u)+L(v)$.\\
	\end{proof}
\end{thm*}
\begin{corollary*}
	For $y>0$, $L(y)= \int_{1}^{y}{\frac{1}{t}\mathrm{d}t}$.
	\begin{proof}
		By Theorem~\ref{thm:ftc},
		$\text{RHS} = L(t)\bigg{\rvert}_{1}^{y}=L(y)-L(1)=L(y)$.
	\end{proof}
\end{corollary*}

\begin{thm*}
	For $p \in \Q$ and $x>0$, $L(x^{p})=pL(x)$, so $x^{p}=e^{p \log{x}}$.
	\begin{proof}
		C.f. Rudin's p.181.
	\end{proof}
\end{thm*}

\begin{define}
	For $\alpha \in \R$, $x>0$, define $x^{\alpha}=e^{\alpha \log{x}}$.
\end{define}
\begin{thm}
	For $x>0$, $\alpha \in \R$, $\frac{\mathrm{d}}{\mathrm{d}x} x^{\alpha}=\alpha x^{\alpha-1}$.
	Hence, $x^{\alpha}$ has an antiderivative of $\begin{cases}
			\frac{x^{\alpha+1}}{\alpha+1}+C & \alpha \neq -1 \\
			\log{x}+C                       & \alpha=-1
		\end{cases}$.
	\begin{proof}
		$\frac{\mathrm{d}}{\mathrm{d}x}x^{\alpha}=\frac{\mathrm{d}}{\mathrm{d}x}e^{\alpha \log{x}}=e^{\alpha \log{x}}\frac{\alpha}{x}=\frac{x^{\alpha}\alpha}{x}=\alpha x^{\alpha-1}$.
	\end{proof}
\end{thm}

\begin{thm}
	\begin{enumerate}
		\item $\lim_{x\to \infty}{\log{x}}=\infty$
		\item $\lim_{x\to 0^{+}}{\log{x}}=-\infty$
		\item $\lim_{x\to \infty}{ \frac{\log{x}}{x^{\alpha}}}=0 \text{ if }  \alpha>0$
	\end{enumerate}.
	\begin{proof}
		\begin{enumerate}
			\item exercise
			\item exercise
			\item For $a>0$, $x>1$, \[
				      \log{x}= \int_{1}^{x}{\frac{1}{t}\mathrm{d}t}\le \int_{1}^{x}{t^{a} \frac{1}{t}\mathrm{d}t}=\frac{1}{a}(x^{a}-1)<\frac{1}{a}x^{a}
				      .\]
			      Choose $a \in (0, \alpha)$. Then \[
				      \frac{1}{x^{\alpha}} \log{x}\le \frac{1}{x^{\alpha}}\frac{1}{a}x^{a} =\frac{1}{a} \frac{1}{x^{\alpha-a}}\to 0 \text{ as } x\to \infty
				      .\]
		\end{enumerate}
	\end{proof}
\end{thm}
For sine,cosine, pi, read Rudin's p. 182-184.

\begin{thm}[8][\namedlabel{thm:fta}{Fundamental Theorem of Algebra}]\\
	Let $n \in \N$, $a_{0},a_{1},a_{2},\ldots ,a_n \in \C$, $a_n\neq 0$,\\
	$P(z)=a_{0}+a_{1}z+a_{2}z^2+\cdots + a_{n-1}z^{n-1}+a_n z^{n}$ for $z \in \C$.\\
	Then there exists $z_{0} \in \C$ such that $P(z_{0})=0$.
	\begin{proof}
		Can assume $a_n=1$. Let $\mu=\inf_{z \in \C}\left|P(z)\right|$.
		We want to show that $\mu=0$ and is attained at some $z_{0} \in \C$.
		\begin{claim}[1]
			There exists $z_{0} \in \C$ such that $\left|P(z_{0})\right|=\mu$.
			\begin{proof}
				Suppose $\left|z\right|=R$. Then
				\[
					\left|P(z)\right|= \left|z\right|^{n} \cdot \left|1+ \frac{a_{n-1}}{z}+ \cdots + \frac{a_{0}}{z^{n}}\right|\ge \left|z\right|^{n} \left(1- \frac{\left|a_{n-1}\right|}{\left|z\right|}- \cdots - \frac{\left|a_0\right|}{\left|z\right|^{n}}\right)=R^{n}\left(1- \frac{\left|a_{n-1}\right|}{R}- \cdots - \frac{\left|a_0\right|}{R^{n}}\right)
					.\]
				Hence, $\left|P(z)\right|\to \infty$ as $R\to \infty$.
				$\left|P(z)\right|\ge \mu+1$ if $\left|z\right|\ge R_{0}$ for some $R_{0}<\infty$, where $R$ depends on $\mu$.\\
				Now $\left|P\right|$ is continuous and $\left|z\right|\le R_{0}$ is compact, so $\mu=\inf_{\left|z\right|\le R_{0}}\left|P(z)\right|=P(z_{0})$ for some $z_{0}$ with $\left|z_{0}\right|\le R_{0}$.
			\end{proof}
		\end{claim}
		\begin{claim}[2]
			$\mu=0$.
			\begin{proof}
				Suppose for contradiction $\mu>0$, so $\mu=P(z_{0})\neq 0$. Let $Q(z)= \frac{P(z_{0}+z)}{P(z_{0})}$. Then $Q(0)=1$, so $Q(z)=1+b_{k}z^{k}+ \cdots + b_n z^{n}$, where $b_k$ is the first non-zero coefficient.\\
				Since $\left|Q(z)\right|\ge \left|Q(0)\right|=1$ for all $z \in \C$.
				On the other hand, $\left|Q(z)\right|\le \left|1+b_k z^{k}\right|+ \sum_{m=k+1}^{n}{\left|b_k\right| \left|z\right|^{k}}$.
				We need to choose $z$ s.t. $\left|1+b_kz^{k}\right|<1$.\\
				Write $b_{k}z^{k}=\left|b_k\right| \frac{b_k}{\left|b_k\right|}z^{k}$, $\frac{b_k}{\left|b_k\right|}=e^{it}$. $z=re^{i\theta}$ to be chosen..\\
				Then $b_k z^{k}=\left|b_k\right|e^{it}r^{k}e^{ik \theta}=\left|b_k\right|r^{k} e^{i(t+k \theta)}$. Choose $\theta$ so that $t+k\theta=\pi$; i.e., $\theta=\frac{\pi-t}{k}$.
				Since $e^{i(t+k \theta)}=-1<0$.
				Choose $r$ small enough that $1+b_k z^{k}=1- \left|b_k\right|r^{k}<1$.
				This contradicts $\left|Q(z)\right|\ge 1$ for all $z \in \C$, so $\mu=0$.
			\end{proof}
		\end{claim}
	\end{proof}
\end{thm}

\section{Fourier Series}
\begin{define}[Fourier Series]
	Given $a<b$ and integrable $f,g: [a,b]\to \C$, we write \[
		<f,g> = \int_{a}^{b}{f(x)\overline{g(x)}\mathrm{d}x}
		.\]
	\begin{remark}
		$<f,g>=\overline{<g,f>}$, $\|f\|_2^{2}= < f,f> = \int_{a}^{b}{\left|f(x)\right|^2\mathrm{d}x}$.
	\end{remark}
	\begin{note}
		\begin{enumerate}
			\item $d(f,g)= \|f-g\|_2$ defines a metric on $\mathscr{C}([a,b])$ but not on $R[a,b]$, as we can have $f \in R[a,b], f\neq 0, \int_{a}^{b}{\left|f(x)\right|^2\mathrm{d}x}=0$.
			\item Sometimes when $[a,b]= [-\pi,\pi]$, we prefer $<f ,g > = \frac{1}{2\pi} \cdot \int_{-\pi}^{\pi}{f(x) \overline{g(x)}\mathrm{d}x}$.
		\end{enumerate}
	\end{note}
\end{define}

\begin{define}[Orthogonal Family of Functions]
	Functions $\phi_n:[a,b]\to \C$ are \textit{orthogonal} if $<\phi_n,\phi_m>=0$ for $n\neq m$.\\
	They are \textit{orthonormal} if $<\phi_m,\phi_n>=\delta_{mn}= \begin{cases}
			1 & m= n    \\
			0 & m\neq n
		\end{cases}$.
\end{define}

\begin{example}
	\begin{enumerate}
		\item $[a,b]=[-\pi,\pi]$, $\phi_n(x)=\frac{1}{\sqrt{2\pi}}e^{i n x}$ for $n \in \Z$ obey $<\phi_m, \phi_n> =\int_{-\pi}^{\pi}{\frac{1}{2\pi}e^{-i(m-n)x}\mathrm{d}x}=\frac{1}{2\pi}\int_{-\pi}^{\pi}{\left[\cos{(m-n)x}+i \sin{(m-n)x}\right] \mathrm{d}x}
			      =\delta_{mn}$. Hence $\{\phi_n\}_{n \in \Z}$ is an orthonormal set.
		\item $\phi_{1},\phi_{2}, \phi_{3}, \ldots  $ given by $\frac{1}{\sqrt{2\pi}}, \frac{1}{\sqrt{\pi}} \cos{(nx)}, \frac{1}{\sqrt{\pi}} \sin{(nx)}, \ldots$ are orthonormal on $[-\pi,\pi]$.
		\item Legendre polynomials $\{P_n(x)\}$ are orthogonal with $\|P_n\|_{2}=\sqrt{\frac{2}{2n+1}}$ on $[-1,1]$:
		      \[
			      P_{n}(x)=\sum_{k=0}^{n}{\binom{n}{k}\binom{n+k}{n} {\left(\frac{x-1}{2}\right)}^{k}} \text{ for } n=0,1,2,\ldots
			      .\]
	\end{enumerate}
\end{example}


\begin{define}[Fourier Coefficients]
	Let $\{\phi_n\}_{n \in \Z}$ be an orthonormal set on $[a,b]$.\\
	Suppose $f(x)=\sum_{m=0}^{N}{c_m \phi_m (x)}$ with $\{\phi_n\}_{n \in \Z}$ orthonormal on $[a,b]$. Then
	\begin{flalign*}
		<f,\phi_n > & = <\sum_{m=0}^{n}{c_{m} \phi_m}, \phi_n > = \sum_{m=0}^{n}{c_m < \phi_m , \phi_n >}= \sum_{m=0}^{n}{c_m \delta_{mn}}=c_n
		.\end{flalign*}
	I.e., $c_n = < f, \phi_{n} > =  \int_{a}^{b}{f(x) \overline{\phi_n(x)}\mathrm{d}x}$.
	E.g., if $f(x)=\sum_{n=-N}^{N}{c_n e^{inx}}=\sum_{n=-N}^{N}{c_{n} \sqrt{2\pi}} \cdot \left(\frac{1}{\sqrt{2\pi}} e^{inx}\right)$, then $\sqrt{2\pi}c_n= < f, \phi_n > =\int_{-\pi}^{\pi}{f(x) \frac{1}{\sqrt{2\pi}} e^{-inx}\mathrm{d}x}$; i.e., $c_n= \frac{1}{2\pi} \int_{-\pi}^{\pi}{f(x) e^{-inx}\mathrm{d}x}$.
\end{define}

Question: When does the Fourier series converge and when does it equal $f$?
\begin{thm}[11]
	Suppose $f \in \mathscr{R}[a,b]$ and $\{\phi_m\}_{m=1,2,\ldots }$ is orthonormal. Let $c_{n}= < f, \phi_m> , s_n= \sum_{m=1}^{n}{c_m \phi_m}$.\\
	Let $t_n= \sum_{m=1}^{n}{a_m \phi_m}$ for some $a_m \in \C$. Then
	\[
		\|f-s_n\|_{2} \le \|f-t_n\|_2
		,\] where the equality holds if and only if $a_m=c_m$ for all $m=1,2,\ldots $.
	\begin{proof}
		${\|f-t_n\|}_{2}^{2}= <f-t_n, f-t_n> = <f,f > - <t_n, f> - <f,t_n> + \underbrace{<t_n, t_n>}_{=\|t_n\|_{2}^{2}}$.
		Note
		\begin{flalign*}
			\|t_n\|_{2}^{2}= <t_n, t_n> = \sum_{k,m=1}^{n}{a_k \overline{a_m} \underbrace{< \phi_k , \phi_m>}_{=\delta_{k,m}}}
			.\end{flalign*}
	\end{proof}
\end{thm}

