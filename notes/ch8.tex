\chapter{Some Special Functions}
Power series, $e^{x}, \log{x},\sin{x},\cos{x}$, Fourier series (we're omitting Gamma function)

\section{Power Series}
Recall $\sum_{n=0}^{\infty}{c_n{x^{n}}}$ has a radius of convergence $R=\frac{1}{\limsup_{n\to \infty}{\sqrt[n]{\left|c_{n}\right| }}}$ such that the series absolutely converges for $\left|x\right| < R$, diverges for $\left|x\right| >R$ and anything possible for $\left|x\right| =R$.

\begin{remark}
	To determine $R$, we often use the ratio test instead:
	\[
		\left|\frac{c_{n+1}x^{n+1}}{c_nx^{n}}\right| =\left|x\right| \cdot \left|\frac{c_{n+1}}{c_n}\right|\underbrace{\to}_{\text{ if limit exists } } \left|x\right| \cdot L\implies \text{ absolute convergence if } \left|x\right| <\frac{1}{L}, \text{ diverges if }  \left|x\right| >\frac{1}{L}
		,\] where $R=\frac{1}{L}=\frac{1}{\lim_{n\to \infty}{\left|\frac{c_{n+1}}{c_n}\right| }}$.\\
	In general, by Theorem~3.37, we also have \[
		\frac{1}{\liminf_{n\to \infty}{\left|\sqrt[n]{c_n}\right|}}\le R\le \frac{1}{\liminf_{n\to \infty}{\frac{c_{n+1}}{c_n}}}
		.\]
\end{remark}

\begin{theorem}[1]
	Suppose $\sum_{n=0}^{\infty}{c_n x^{n}}$ has a radius of convergence $R>0$ and define $f(x)=\sum_{n=0}^{\infty}{c_n x^{n}}$ for $\left|x\right| < R$. Such a function $f(x)$ is called an \textit{analytic function} .\\
	If $R<\infty$, then the series converges uniformly on $[-R+\epsilon,R-\epsilon]$ for all $\epsilon>0$.\\
	If $R=\infty$, then the series converges uniformly on $[-M,M]$ for all $M < \infty$.
	The function $f$ is continuous and differentiable on $(-R,R)$ with $f'(x)=\sum_{n}{c_n n x^{n-1}}$
	\begin{note}
		a function of the form $f(x)=\sum_{n=0}^{\infty}{c_n x^{n}}$ is called an \textit{analytic function} .\\
	\end{note}
	\begin{remark}
		Uniform convergence may not hold on $(-R,R)$. C.f. A7-Q3.
	\end{remark}
	\begin{proof}
		\hfill
		\begin{description}
			\item[Uniform convergence:]
			      For $\left|x\right| \le R-\epsilon$, $\left|c_n x^{n}\right|\le \left|c_n\right| (R-\epsilon)^{n}$.\\
			      $\sum_{n=0}^{\infty}{\left|c_n\right|(R-\epsilon)^{n}}<\infty$ by absolute convergence on $(-R,R)$. Hence, by \ref{thm:M-test}, $\sum_{n=0}^{\infty}{c_n x^{n}}$ converges uniformly on $[-R+\epsilon,R-\epsilon]$.\\
			\item[Derivative:]
			      The radius of convergence of $\sum_{n=1}^{\infty}{nc_n x^{n-1}}$ is
			      \begin{flalign*}
				      \frac{1}{\limsup_{n\to \infty}{\sqrt[n]{n \left|c_n\right| }}} & =\frac{1}{\limsup_{n\to \infty}{\sqrt[n]{n}\sqrt[n]{\left|c_n\right|}}}                                                      \\
				                                                                     & =R =\text{ radius of convergence of } \sum_{n=0}^{\infty}{c_n x^{n}} \text{  ($\because \lim_{n\to \infty}{\sqrt[n]{n}}=1$)}
				      .\end{flalign*}
			      Let $S_n(x)=\sum_{m=0}^{n}{c_m x^{m}}$. Then $S'_n(x)=\sum_{m=1}^{n}{c_m m x^{m-1}}$.
			      By the first part of the proof, $S'_n(x)\to \sum_{m=1}^{\infty}{c_m m x^{m-1}}$ uniformly on $[-R+\epsilon, R-\epsilon]$. Since also $S_n(x)\to f(x)$, by Theorem~\ref{thm:7.17}, $f'$ exists on $[-R+\epsilon,R-\epsilon]$, and $f'(x)=\sum_{m=1}^{\infty}{c_m m x^{m-1}}$.\\
			      Since $\epsilon$ is arbitrary, $f'(x)=\sum_{m=1}^{\infty}{c_m m x^{m-1}}$ for all $x \in (-R,R)$. In particular, $f$ is also continuous.
		\end{description}
	\end{proof}
\end{theorem}

\begin{corollary}
	If $f(x)=\sum_{n=0}^{\infty}{c_n x^{n}}$ converges for $\left|x\right| < R$, then $f^{(k)}(x)$ exists for all $k \in \N$, and
	\begin{equation*}
		f^{(k)}(x)=\sum_{n=k}^{\infty}{c_n n(n-1)\cdots (n-k+1)x^{n-k}}
		\tag{*}
		.\end{equation*}
	Consequently, $c_k=f^{(k)}(0)$ and $f(x)=\sum_{n=0}^{\infty}{\frac{f^{(n)}(0)}{n!} x^{n}}$.
	\begin{note}
		C.f. Taylor's theorem.
	\end{note}
	\begin{proof}
		By Theorem~\ref{thm:8.1}, $f'(x)=\sum_{n=1}^{\infty}{c_n n x^{n-1}}, f''(x)=(f')'(x)=\sum_{n=2}^{\infty}{c_n n(n-1)x^{n-2}}, \ldots $.\\
		Set $x=0$ in (*) to get $f^{(k)}(0)\underbrace{=}_{\text{ only } n=k \text{ term survives } }c_k k (k-1) \cdots \cdot 1=c_k\cdot  k!$.
	\end{proof}
\end{corollary}
\begin{example}
	Let $f(x)=\begin{cases}
			e^{-\frac{1}{x^2}} & x\neq 0 \\
			0                  & x=0
		\end{cases}$.
	By Rudin's problem 8.1, $f^{(n)}(0)=0$ for all $n=0,1,2,\ldots $, so
	$f(x)\neq \sum_{n=0}^{\infty}{\frac{f^{(n)}}{n!} x^{n}}$ except for $x=0$.
	\begin{remark}[Bump Function]
		\textit{Bump functions} are infinitely differentiable functions with compact support. E.g.,
		\[
			f(x)=\begin{cases}
				e^{-\frac{1}{1-x^2}} & x \in (-1,1)         \\
				0                    & \left|x\right| \ge 1
			\end{cases}
			.\]
	\end{remark}
\end{example}

\begin{thm}[2][\namedlabel{thm:abel}{Abel's Theorem}]
	Suppose $\sum_{n=0}^{\infty}{c_n}$ converges (perhaps conditionally). Let $f(x)=\sum_{n=0}^{\infty}{c_n x^{n}}$.
	Then $f(x)$ converges for $\left|x\right| < 1$ and $\lim_{x\to 1^{-}}{f(x)}=f(1)=\sum_{n=0}^{\infty}{c_n}$.
	\begin{remark}
		Interesting case is at $R=1$, since $R>1$ implies continuity of $f$ for $\left|x\right| < R$.
	\end{remark}
	\begin{proof}
		By the root test, $\limsup_{n\to \infty}{\sqrt[n]{\left|c_n\right|}}\le 1$, so $\sum_{n=0}^{\infty}{c_n x^{n}}$ has $R\ge 1$.
		Let $S_n=\sum_{m=0}^{n}{c_m}$ and $S=\sum_{m=0}^{\infty}{c_m}=\lim_{n\to \infty}{S_n}$.\\
		Set $S_{-1}=0$. Then $c_{n}=s_{n}-s_{n-1}$ for $n\ge 0$.\\
		Let $\epsilon>0$. We need to show $\exists{\delta > 0} \text{ such that } 1-\delta<x<1 \implies \left|f(x)-S\right| < \epsilon$.\\
		Start with partial sum for $f(x)$. For $\left|x\right| < 1$,
		\begin{flalign*}
			\sum_{m=0}^{n}{c_m x^{m}}=\sum_{m=0}^{n}{(S_m-S_{m-1})x^{m}}=\sum_{m=0}^{n}{S_m x^{m}}-\sum_{m=0}^{n}{S_{m-1}x^{m}}
			.\end{flalign*}
		Let $k=m-1$ so that $m=k+1$. Then
		\begin{flalign*}
			 & \sum_{m=0}^{n}{S_{m-1}x^{m}} =\underbrace{S_{0-1}}_{0}\cdot 1+\sum_{m=1}^{n}{S_{m-1}x^{m}} =x\sum_{k=0}^{n-1}{S_k x^{k}}                                                        \\
			 & \sum_{m=0}^{n}{c_m x^{m}}     =(1-x)\sum_{m=0}^{n}{S_m x^{m}}+\underbrace{S_n x^{n+1}}_{\to 0 \text{ as } n\to \infty \text{ since $S_n$ is bounded and $\left|x\right| < 1$} }
			.\end{flalign*}
		Let $n\to \infty$.
		Then
		\begin{flalign*}
			f(x) & =(1-x)\sum_{n=0}^{\infty}{S_n x^{n}}
			.\end{flalign*}
		\begin{flalign*}
			\left|f(x)-S\right| & =\left|(1-x)\sum_{n=0}^{\infty}{S_n x^{n}}-S(1-x) \cdot \frac{1}{1-x}\right|
			=	\left|(1-x)\sum_{n=0}^{\infty}{S_n x^{n}}-S(1-x)\sum_{n=0}^{\infty}{x^{n}}\right|                \\                                                                                   & =(1-x) \left|\sum_{n=0}^{\infty}{(S_n-S)x^{n}}\right|\le (1-x)\sum_{n=0}^{\infty}{\left|S_n-S\right|\cdot \left|x \right|^{n}  }
			.\end{flalign*}
		Choose $N$ s.t. $n\ge N \implies \left|S_n-S\right|<\frac{\epsilon}{2} $.
		For $x \in (0,1)$,
		\begin{flalign*}
			\left|f(x)-S\right| & \le (1-x) \sum_{n=0}^{N}{\left|S_n-S\right| x^{n}}+(1-x) \sum_{n=N+1}^{\infty}{\left|S_n-S\right|x^{n}}     \\
			                    & <(1-x) \sum_{n=0}^{N}{\left|S_n-S\right| x^{n}} +(1-x)\left( \frac{\epsilon}{2} \cdot \frac{1}{1-x}\right)=
			(1-x) \left(\sum_{n=0}^{N}{\left|S_n-S\right| x^{n}}\right)+\frac{\epsilon}{2}
			.\end{flalign*}
		Since $(1-x)\sum_{n=0}^{N}{\left|S_n-S\right| x^{n}}$ is a polynomial in $x$, so it is continuous and equals $0$ at $x=1$.\\
		Hence, $(1-x)\sum_{n=0}^{N}{\left|S_n-S\right| x^{n}}<\frac{\epsilon}{2}$ if $\left|x-1\right| <\delta$ for some $\delta>0$. Therefore, for $1-\delta<x<1$, $\left|f(x)-S\right|<\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon$.
	\end{proof}
	\begin{note}
		For an application of Abel's theorem, see Rudin's p. 175.\\
		For the case $\sum_{n=0}^{\infty}{c_n}=\infty$, see A7.
	\end{note}
\end{thm}

\begin{thm}[3]
	If $\sum_{i=1}^{\infty}{\sum_{j=1}^{\infty}{\left|a_{ij}\right|}}$ converges, then \[
		\sum_{i=1}^{\infty}{\sum_{j=1}^{\infty}{a_{ij}}}= \sum_{j=1}^{\infty}{\sum_{i=1}^{\infty}{a_{ij}}},\] where both sides converge.
	\begin{proof}
		Rudin has a too clever proof \ldots  A7 involves a more straightforward proof.
	\end{proof}
\end{thm}

\begin{thm}[4]
	Suppose $f(x)=\sum_{n=0}^{\infty}{c_n x^{n}}$ (Taylor series of $f$ at $x=0$, a.k.a Maclauren series) has a radius of convergence $R>0$.
	Let $\left|a\right|<R$.
	Then $f(x)=\sum_{n=0}^{\infty}{\frac{f^{(n)}(a)}{n!}(x-a)^{n}}$ for (at least) $\left|x-a\right|<R-\left|a\right|$.
	\begin{proof}
		Note \[
			f(x)=\sum_{n=0}^{\infty}{c_n \left( a+(x-a) \right)^{n}}=\sum_{n=0}^{\infty}{c_n}\sum_{m=0}^{n}{\binom{n}{m}(x-a)^{m}a^{n-m}}
			.\]
		We want to interchange the order of summation.\\
		By Theorem~\ref{thm:8.3}, interchange of summations is justified if \[
			\sum_{n=0}^{\infty}{\sum_{m=0}^{n}{\left|c_n\right|\binom{n}{m}\left|x-a\right|^{m}\left|a\right|^{n-m}}}<\infty
			.\]
		Note $\sum_{n=0}^{\infty}{\left|c_{n}\right|\left( \left|x-a\right|+\left|a\right|\right)^{n}}$ does converge as we assume $\left|x-a\right|+\left|a\right|<R$.\\
		Therefore, for $\left|x-a\right| < R-\left|a\right|$,
		\begin{flalign*}
			f(x) & =\sum_{m}^{\infty}\frac{1}{m!}(x-a)^{m}{\sum_{n=m}^{\infty}{c_n[ n\cdot (n-1)(n-2)\cdots \cdot (n-m+1)] a^{n-m}}} \\
			     & =\sum_{m}{\left(\sum_{n}{c_n \binom{n}{m}a^{n-m}}\right)(x-a)^{m}}                                                \\
			     & =\sum_{m}{\left(\sum_{n}{\frac{f^{(n)}(a)}{n!}n(n-1)\cdots (n-m+1)a^{n-m}}\right)(x-a)^{m}}                       \\
			.\end{flalign*}
	\end{proof}
	\begin{example}
		Let $f(x)=\sum_{n=0}^{\infty}{x^{n}}$ for $\left|x\right|<1 $. Then $f(x)=\frac{1}{1-x}$ for $\left|x\right|<1$.\\
		Taylor series of $\frac{1}{1-x}$ at $x=-\frac{1}{2}$:
		For $\left|x\right| < 1$, $f^{(n)}(x)=\frac{n!}{(1-x)^{n+1}}$, so $f^{(n)}(-\frac{1}{2})=\frac{n!}{(\frac{3}{2})^{n+1}}$.\\
		By Theorem~\ref{thm:8.4},
		\[
			f(x)=\sum_{n=0}^{\infty}{\frac{n!}{(\frac{3}{2})^{n+1}n!}\left(x+\frac{1}{2}\right)^{n}}=\sum_{n=0}^{\infty}{(\frac{2}{3})^{n}\left(x+\frac{1}{2}\right)^{n}}
		\]
		for $\left|x+\frac{1}{2}\right|<1-\left|-\frac{1}{2}\right| =\frac{1}{2}$.\\
		In fact, the series converges even when $\left|\frac{2}{3}\left(x+\frac{1}{2}\right)\right|<1$; i.e., $\left|x+\frac{1}{2}\right| <\frac{3}{2}$. C.f. Analytic continuation.\\
		Another way to get Taylor series at $x=-\frac{1}{2}$:\\
		For $\left|x\right|<1 $,
		\[
			f(x)=\frac{1}{1-x}=\frac{1}{\left(1+\frac{1}{2}\right)-\left(x+\frac{1}{2}\right)}=\frac{2}{3} \frac{1}{1-\frac{2}{3}\left(x+\frac{1}{2}\right)}=\frac{2}{3}\sum_{n=0}^{\infty}{\left(\frac{2}{3}\right)^{n}\left(x+\frac{1}{2}\right)^n}.
		\]
	\end{example}
\end{thm}

\begin{thm}[5][\namedlabel{thm:pop}{Principle of Permanence of Form}]
	Suppose $\sum_{n}{a_{n}x^{n}}$ and $\sum_{n}{b_{n}x^{n}}$ have radii of convergence at least  $R$. Suppose $D \subset (-R,R)$ has a limit point in $(-R,R)$.\\
	If $\sum_{n}{a_{n}x^{n}}=\sum_{n}{b_{n}x^{n}}$ for all $x \in D$, then $a_{n}=b_{n}$ for all $n$, and hence $\sum_{n}{a_{n}x^{n}}=\sum_{n}{b_{n}x^{n}}$ for all $\left|x\right|<R$.
	\begin{proof}
		Let $c_{n}=a_{n}-b_{n}$ and $f(x)=\sum_{n=0}^{\infty}{c_n x^{n}}$.
		Then $f(x)=0$ for all $x \in D$.\\
		Let $E=\{x \in (-R,R): f(x)=0\}$. Then $D \subset E$.\\
		We want to show $E=(-R,R)$.
		Let $A=E' \cap (-R,R)$. Then $A\neq \emptyset$ because $D$ has a limit point in $(-R,R)$.\\
		Also, relative to $(-R,R)$, $A$ is closed as the set of all limit points is always closed (C.f. Problem 2.6).\\
		Let $B=(-R,R)\setminus A$. Then $A\cup B=(-R,R)$, $A\cap B=\emptyset$, and $B$ is open.\\
		\begin{claim}
			$A$ is also open.
			\begin{proof}
				Let $x_{0} \in A=E' \cap (-R,R)$.
				Then $\exists{\{ {d}_{n}\}} \text{ such that } f(x)=\sum_{n=0}^{\infty}{d_n (x-x_{0})}^{n}$ for $\left|x-x_{0}\right|<R-\left|x_{0}\right|$.
				If we show that $d_{n}=0$ for all $n\ge 0$, it proves $(x_{0}-r,x_{0}+r) \subset A$, and hence that $A$ is open.
				Suppose for contradiction there exists $k\ge 0$ such that $d_k\neq 0$ and $f(x)=\sum_{n=k}^{\infty}{d_n (x-x_{0})^{n}}$.
				Then $f(x)=(x-x_{0})^{k} \sum_{n=k}^{\infty}{d_n(x-x_{0})^{n-k}}$.
				Let $m=n-k$. Then \[
					f(x)=(x-x_{0})^{k} \sum_{m=0}^{\infty}{d_{m+k}(x-x_{0})^{m}}
					.\]
				Say $g(x)=\sum_{m=0}^{\infty}{d_{m+k}(x-x_{0})^{m}}$. Then
				$g(x_0)=d_k\neq 0$, and by continuity of $g$, $\exists{\delta>0} \text{ such that } g(x)\neq 0$ if $\left|x-x_{0}\right|<\delta$.
				However, $f(x)=(x-x_{0})^{k}g(x)\neq 0	 \text{ if } 0<\left|x-x_{0}\right|<\delta$, so $x_{0}$ is an isolated zero of $f$ and $x_{0} \in E'$.
				This is a contradiction, so $d_{n}=0$ for all $n\ge 0$.
			\end{proof}
		\end{claim}
		Given the claim, in $(-R,R)$, $A$ and $B$ are both open and closed.
		Since $(-R,R)$ is connected (C.f. MATH-320's A5.3(c)),
		one of \begin{enumerate}[label=(\roman*)]
			\item $A=\emptyset,B=(-R,R)$
			\item $A=(-R,R),B=\emptyset$
		\end{enumerate}
		must hold.\\
		As $A$ is non-empty, $A=(-R,R)=E'$.\\
		Therefore, $\forall{x \in (-R,R)}: \exists{\{ {x}_{n}\} \text{ in } E} \text{ such that } x_n\to x$.
		Since $f$ is continuous on $(-R,R)$, $f(x)=\lim_{n\to \infty}{f(x_n)}=0$.\\
		Therefore, for all $x \in (-R,R)$, $x \in E$, so $(-R,R)\subset E$.\\
	\end{proof}
\end{thm}

\begin{remark}
	CUT-OFF for MT2 ends here, including A4,5,6,7 and up to Rudin's p.143-178.
\end{remark}



\section{Exponential Function}
Recall \[
	e:=\sum_{n=0}^{\infty}{\frac{1}{n!}}
	.\]

Also, $e=\lim_{n\to \infty}{\left(1+\frac{1}{n}\right)^{n}}$, $e <3, e \in \Q$.
\begin{define}
	For $z \in \C$, $E(z):=\sum_{n=0}^{\infty}{\frac{z^{n}}{n!}}$ and it has a radius of convergence $R=\infty$.
	Note $E(1)=e$ is given by the definition of $e$.
\end{define}

\begin{thm}
	For $p \in \Q$, $E(p)=e^{p}$.
	\begin{proof}
		\begin{enumerate}[label=(\roman*)]
			\item
			      Note \begin{flalign*}
				      E(w)E(z) & =\sum_{m=0}^{\infty}{\frac{w^{m}}{m!}} \sum_{n=0}^{\infty}{\frac{z^{n}}{n!}}= \sum_{n=0}^{\infty}{\sum_{k=0}^{n}{\frac{z^{k}}{k!} \frac{w^{n-k}}{(n-k)!}}} \;\; (\text{ by Theorem~\ref{thm:3.50}}) \\
				               & =\sum_{n=0}^{\infty}{\frac{1}{n!} \sum_{k=0}^{n}{\binom{n}{k}z^{k}w^{n-k}}}=\sum_{n=0}^{\infty}{\frac{(z+w)^{n}}{n!}}=E(z+w).
			      \end{flalign*}
			      In particular,
			      \begin{flalign*}
				      E(2) & =E(1+1)=E(1)^{2}=e^2                                        \\
				      E(3) & =E(1+2)=E(1)E(2)=e\cdot e^2=e^3                             \\
				      \vdots
				      E(n) & =e^n \text{ for } n =0, 1,2,3,\ldots, (E(0)=\frac{1}{0!}=1)
				      .\end{flalign*}
			\item  Since $E(z)E^{-z}=E(z-z)=1$, $E(-z)=\frac{1}{E(z)}$.
			      Hence, $E(-n)=\frac{1}{E(n)}=\frac{1}{e^n}=e^{-n}$.\\
			      Thus, $E(n)=e^{n}$ for $n \in \Z$.
			\item Let $p=\frac{m}{n}$, $m \in \Z, n \in \N$. Then
			      \begin{flalign*}
				      e^{m}=E(m)=E(np)=E(p+p+\cdots +p)=E(p)^{n}
				      .\end{flalign*}
			      Hence, $E(\frac{m}{n})=E(p)=e^{\frac{m}{n}}=e^{p}$.
		\end{enumerate}
	\end{proof}
\end{thm}

\begin{define}
	For $x \in \R$, let $e^{x}=\sum_{n=0}^{\infty}{\frac{x^{n}}{n!}}=E(x)$.
	This defines $x \mapsto e^{x}$ as an analytic function on $\R$, which must be infinitely differentiable and \[
		\frac{\mathrm{d}}{\mathrm{d}x}e^{x}=\sum_{n=1}^{\infty}{\frac{1}{n!}n x^{n-1}}= \sum_{n=1}^{\infty}{\frac{1}{(n-1)!} x^{n-1}}=e^{x}
		.\]
	We then have $\frac{\mathrm{d}^{k}}{\mathrm{d}x^{k}}e^{x}=e^{x}$ for all $x=0,1,2,\ldots $.\\
	Since $e^{x}e^{-x}=1$ and since $e^{x}>0$ for $x\ge 0$, $e^{-x}>0$, so $e^{x}>0$ for all $x \in \R$.\\
	From the first two derivatives, $x \mapsto e^{x}$ is strictly increasing and strictly convex.\\
	For $n\ge 0$ and $x>0$, $e^{x}> \frac{x^{n+1}}{(n+1)!}$, so $\frac{e^{x}}{x^{n}}>\frac{x}{(n+1)!}\to \infty$ as $x\to \infty$. \\
	In particular, with $n=0$, $e^{x}\to \infty$ as $x\to \infty$, and $e^{-x}=\frac{1}{e^{x}}\to 0$ as $x\to \infty$.
\end{define}

\begin{define}
	For $y>0$, define $L(y)=\log{y}(= \ln{y})$ by $E(L(y))=y$.
	Equivalently, $L(E(x))=x$ for all $x \in \R$, or
	$e^{\log{y}}=y$ for all $y>0$ and $\log{e^{x}}=x$.\\
	Since $x \mapsto e^{x}$ is strictly increasing and differentiable,
	its inverse $y \mapsto \log{y}$ is also strictly increasing and differentiable.\\
\end{define}
\begin{thm*}
	$L'(y)=\frac{1}{y}$ for $y>0$, $L(uv)=L(u)+L(v)$ for $u,v>0$.
	\begin{proof}
		Apply the chain rule to $L(E(x))=x$ (C.f. Rudin's Problem 5.2).\\
		Then $L'(E(x))E'(x)=1$, so $L'(y)=L'(E(x))=\frac{1}{E(x)}=\frac{1}{y}$.\\
		Also, since there exist unique $x,y$ such that $E(x)=u,E(y)=v$, $L(uv)=L(E(x)E(y))=L(E(x+y))=x+y=L(u)+L(v)$.\\
	\end{proof}
\end{thm*}
\begin{corollary*}
	For $y>0$, $L(y)= \int_{1}^{y}{\frac{1}{t}\mathrm{d}t}$.
	\begin{proof}
		By Theorem~\ref{thm:ftc},
		$\text{RHS} = L(t)\bigg{\rvert}_{1}^{y}=L(y)-L(1)=L(y)$.
	\end{proof}
\end{corollary*}

\begin{thm*}
	For $p \in \Q$ and $x>0$, $L(x^{p})=pL(x)$, so $x^{p}=e^{p \log{x}}$.
	\begin{proof}
		C.f. Rudin's p.181.
	\end{proof}
\end{thm*}

\begin{define}
	For $\alpha \in \R$, $x>0$, define $x^{\alpha}=e^{\alpha \log{x}}$.
\end{define}
\begin{thm}
	For $x>0$, $\alpha \in \R$, $\frac{\mathrm{d}}{\mathrm{d}x} x^{\alpha}=\alpha x^{\alpha-1}$.
	Hence, $x^{\alpha}$ has an antiderivative of $\begin{cases}
			\frac{x^{\alpha+1}}{\alpha+1}+C & \alpha \neq -1 \\
			\log{x}+C                       & \alpha=-1
		\end{cases}$.
	\begin{proof}
		$\frac{\mathrm{d}}{\mathrm{d}x}x^{\alpha}=\frac{\mathrm{d}}{\mathrm{d}x}e^{\alpha \log{x}}=e^{\alpha \log{x}}\frac{\alpha}{x}=\frac{x^{\alpha}\alpha}{x}=\alpha x^{\alpha-1}$.
	\end{proof}
\end{thm}

\begin{thm}
	\begin{enumerate}
		\item $\lim_{x\to \infty}{\log{x}}=\infty$
		\item $\lim_{x\to 0^{+}}{\log{x}}=-\infty$
		\item $\lim_{x\to \infty}{ \frac{\log{x}}{x^{\alpha}}}=0 \text{ if }  \alpha>0$
	\end{enumerate}.
	\begin{proof}
		\begin{enumerate}
			\item exercise
			\item exercise
			\item For $a>0$, $x>1$, \[
				      \log{x}= \int_{1}^{x}{\frac{1}{t}\mathrm{d}t}\le \int_{1}^{x}{t^{a} \frac{1}{t}\mathrm{d}t}=\frac{1}{a}(x^{a}-1)<\frac{1}{a}x^{a}
				      .\]
			      Choose $a \in (0, \alpha)$. Then \[
				      \frac{1}{x^{\alpha}} \log{x}\le \frac{1}{x^{\alpha}}\frac{1}{a}x^{a} =\frac{1}{a} \frac{1}{x^{\alpha-a}}\to 0 \text{ as } x\to \infty
				      .\]
		\end{enumerate}
	\end{proof}
\end{thm}
For sine,cosine, pi, read Rudin's p. 182-184.

\begin{thm}[8][\namedlabel{thm:fta}{Fundamental Theorem of Algebra}]\\
	Let $n \in \N$, $a_{0},a_{1},a_{2},\ldots ,a_n \in \C$, $a_n\neq 0$,\\
	$P(z)=a_{0}+a_{1}z+a_{2}z^2+\cdots + a_{n-1}z^{n-1}+a_n z^{n}$ for $z \in \C$.\\
	Then there exists $z_{0} \in \C$ such that $P(z_{0})=0$.
\end{thm}
\begin{proof}
	Can assume $a_n=1$. Let $\mu=\inf_{z \in \C}\left|P(z)\right|$.
	We want to show that $\mu=0$ and is attained at some $z_{0} \in \C$.
	\begin{claim}[1]
		There exists $z_{0} \in \C$ such that $\left|P(z_{0})\right|=\mu$.
	\end{claim}
	\begin{proof}
		Suppose $\left|z\right|=R$. Then
		\begin{flalign*}
			\left|P(z)\right| & = \left|z\right|^{n} \cdot \left|1+ \frac{a_{n-1}}{z}+ \cdots + \frac{a_{0}}{z^{n}}\right|                                               \\
			                  & \ge \left|z\right|^{n} \left(1- \frac{\left|a_{n-1}\right|}{\left|z\right|}- \cdots - \frac{\left|a_0\right|}{\left|z\right|^{n}}\right) \\
			                  & =R^{n}\left(1- \frac{\left|a_{n-1}\right|}{R}- \cdots - \frac{\left|a_0\right|}{R^{n}}\right)
			.\end{flalign*}
		Hence, $\left|P(z)\right|\to \infty$ as $R\to \infty$, so there exists $R_{0}$ that depends on $\mu$ such that $\left|P(z)\right|\ge \mu$ if $\left|z\right|\ge R_{0}$.\\
		Since $\left|P\right|$ is continuous and $\left|z\right|\le R_{0}$ is compact, by Theorem~\ref{thm:4.16}, $\mu=\inf_{\left|z\right|\le R_{0}}\left|P(z)\right|=P(z_{0})$ for some $z_{0}$ with $\left|z_{0}\right|\le R_{0}$.
	\end{proof}
	\begin{claim}[2]
		$\mu=0$.
	\end{claim}
	\begin{proof}
		Suppose for contradiction $\mu>0$, so $\mu=P(z_{0})\neq 0$.
		Let
		\[
			Q(z)= \frac{P(z_{0}+z)}{P(z_{0})}
			.\]
		As we assume $P(z_{0})=\mu > 0$, $Q(z)$ is well defined.\\
		As $Q(0)=1$, we can write $Q(z)=1+b_{k}z^{k}+ \cdots + b_n z^{n}$, where $b_k$ is the first non-zero coefficient.\\
		By definition of $z_{0}$, for all $z \in \C$,
		\[
			\left|P(z_{0}+z)\right|\ge P(z_{0})=\mu
			.\]
		Hence, \[
			\left|Q(z)\right|\ge \left|Q(0)\right|=1
			.\]
		Note
		\[
			\left|Q(z)\right|\le \left|1+b_k z^{k}\right|+ \sum_{m=k+1}^{n}{\left|b_m\right| \left|z\right|^{m}}
			.\]
		We need to choose $z$ s.t. $\left|1+b_kz^{k}\right|<1$.\\
		Write $b_{k}z^{k}=\left|b_k\right| \frac{b_k}{\left|b_k\right|}z^{k}$, $\frac{b_k}{\left|b_k\right|}=e^{it}$. $z=re^{i\theta}$ to be chosen..\\
		Then
		\[
			b_k z^{k}=\left|b_k\right|e^{it}r^{k}e^{ik \theta}=\left|b_k\right|r^{k} e^{i(t+k \theta)}
			.\]
		Let $\theta$ so that $t+k\theta=\pi$; i.e., $\theta=\frac{\pi-t}{k}$.
		Let $\epsilon=1+b_k z^{k}=1- \left|b_k\right|r^{k}$.
		Then $\epsilon<1$, and for a small enough $r$, we have
		\[
			\sum_{m=k+1}^{n}{\left|b_m\right|r^{m}}<1-\epsilon
			.\]
		Therefore, there exists $z =re^{i \theta}\in \C$ such that
		\[
			\left|Q(z)\right|=\left|Q(re^{i\theta})\right|\le 1-\left|b_k\right|^{r^{k}}+ \sum_{m=k+1}^{n}{\left|b_m\right|r^{m}}=\epsilon+ \sum_{m=k+1}^{n}{\left|b_m\right|r^{m}}<1
			.\]
		This contradicts $\left|Q(z)\right|\ge 1$ for all $z \in \C$, so $\mu=0$.
	\end{proof}
\end{proof}
\section{Fourier Series}
\begin{define}[Fourier Series]
	Given $a<b$ and integrable $f,g: [a,b]\to \C$, we write \[
		<f,g> = \int_{a}^{b}{f(x)\overline{g(x)}\mathrm{d}x}
		.\]
	\begin{remark}
		$<f,g>=\overline{<g,f>}$, $\|f\|_2^{2}= < f,f> = \int_{a}^{b}{\left|f(x)\right|^2\mathrm{d}x}$.
	\end{remark}
	\begin{note}
		\begin{enumerate}
			\item $d(f,g)= \|f-g\|_2$ defines a metric on $\mathscr{C}([a,b])$ but not on $\mathscr{R}[a,b]$, as we can have $f \in \mathscr{R}[a,b], f\neq 0, \int_{a}^{b}{\left|f(x)\right|^2\mathrm{d}x}=0$.
			\item Sometimes when $[a,b]= [-\pi,\pi]$, we prefer $<f ,g > = \frac{1}{2\pi} \cdot \int_{-\pi}^{\pi}{f(x) \overline{g(x)}\mathrm{d}x}$.
		\end{enumerate}
	\end{note}
\end{define}

\begin{define}[Orthogonal Family of Functions]
	Functions $\phi_n:[a,b]\to \C$ are \textit{orthogonal} if $<\phi_n,\phi_m>=0$ for $n\neq m$.\\
	They are \textit{orthonormal} if $<\phi_m,\phi_n>=\delta_{mn}= \begin{cases}
			1 & m= n    \\
			0 & m\neq n
		\end{cases}$.
\end{define}

\begin{example}
	\begin{enumerate}
		\item $[a,b]=[-\pi,\pi]$, $\phi_n(x)=\frac{1}{\sqrt{2\pi}}e^{i n x}$ for $n \in \Z$ obey $<\phi_m, \phi_n> =\int_{-\pi}^{\pi}{\frac{1}{2\pi}e^{-i(m-n)x}\mathrm{d}x}=\frac{1}{2\pi}\int_{-\pi}^{\pi}{\left[\cos{(m-n)x}+i \sin{(m-n)x}\right] \mathrm{d}x}
			      =\delta_{mn}$. Hence $\{\phi_n\}_{n \in \Z}$ is an orthonormal set.
		\item $\phi_{1},\phi_{2}, \phi_{3}, \ldots  $ given by $\frac{1}{\sqrt{2\pi}}, \frac{1}{\sqrt{\pi}} \cos{(nx)}, \frac{1}{\sqrt{\pi}} \sin{(nx)}, \ldots$ are orthonormal on $[-\pi,\pi]$.
		\item Legendre polynomials $\{P_n(x)\}$ are orthogonal with $\|P_n\|_{2}=\sqrt{\frac{2}{2n+1}}$ on $[-1,1]$:
		      \[
			      P_{n}(x)=\sum_{k=0}^{n}{\binom{n}{k}\binom{n+k}{n} {\left(\frac{x-1}{2}\right)}^{k}} \text{ for } n=0,1,2,\ldots
			      .\]
	\end{enumerate}
\end{example}


\begin{define}[Fourier Coefficients]
	Let $\{\phi_n\}_{n \in \Z}$ be an orthonormal set on $[a,b]$.\\
	Suppose $f(x)=\sum_{m=0}^{N}{c_m \phi_m (x)}$ with $\{\phi_n\}_{n \in \Z}$ orthonormal on $[a,b]$. Then
	\begin{flalign*}
		<f,\phi_n > & = <\sum_{m=0}^{N}{c_{m} \phi_m}, \phi_n > = \sum_{m=0}^{N}{c_m < \phi_m , \phi_n >}= \sum_{m=0}^{N}{c_m \delta_{mn}}=c_n
		.\end{flalign*}
	I.e., $c_n = < f, \phi_{n} > =  \int_{a}^{b}{f(x) \overline{\phi_n(x)}\mathrm{d}x}$.
	E.g., if $f(x)=\sum_{n=-N}^{N}{c_n e^{inx}}=\sum_{n=-N}^{N}{c_{n} \sqrt{2\pi}} \cdot \left(\frac{1}{\sqrt{2\pi}} e^{inx}\right)$, then $\sqrt{2\pi}c_n= < f, \phi_n > =\int_{-\pi}^{\pi}{f(x) \frac{1}{\sqrt{2\pi}} e^{-inx}\mathrm{d}x}$; i.e., $c_n= \frac{1}{2\pi} \int_{-\pi}^{\pi}{f(x) e^{-inx}\mathrm{d}x}$.
\end{define}

Question: When does the Fourier series converge and when does it equal $f$?
\begin{thm}[11]
	Suppose $f \in \mathscr{R}[a,b]$ and $\{\phi_m\}_{m=1,2,\ldots }$ is orthonormal. Let $c_{n}= < f, \phi_n> , s_n= \sum_{m=1}^{n}{c_m \phi_m}, t_n= \sum_{m=1}^{n}{a_m \phi_m}$ for some $a_m \in \C$. Then
	\[
		\|f-s_n\|_{2} \le \|f-t_n\|_2
		,\] where the equality holds if and only if $a_m=c_m$ for all $m=1,2,\ldots $.
	\begin{proof}
		${\|f-t_n\|}_{2}^{2}= <f-t_n, f-t_n> = <f,f > - <t_n, f> - <f,t_n> + \underbrace{<t_n, t_n>}_{=\|t_n\|_{2}^{2}}$.
		Note
		\begin{flalign*}
			 & \|t_n\|_{2}^{2}  = <t_n, t_n> = \sum_{k,m=1}^{n}{a_k \overline{a_m} \underbrace{< \phi_k , \phi_m>}_{=\delta_{k,m}}}=\sum_{m=1}^{n}{a_m \overline{a_m}}                                                                                 \tag{*} \\
			 & <f,t_n>          = \sum_{m=1}^{n}{\overline{a_m} \underbrace{< f, \phi_m > }_{=c_m}}=\sum_{m=1}^{n}{\overline{a_m}c_m}
			.\end{flalign*}
		\begin{flalign*}
			\|f-t_n\|_{2}^{2} & = \|f\|_{2}^{2}+ \sum_{m=1}^{n}{\underbrace{a_m \overline{a}_m - \overline{a}_mc_m - a_m \overline{c}_m+c_m\overline{c}_m}_{=\left|a_m-c_m\right|^2}}- \underbrace{\sum_{m=1}^{n}{c_m \overline{c}_m}}_{=\|s_n\|_{2}^2 \text{ by } (*)}
			.\end{flalign*}
		Thus, $\|f-t_n\|_{2}^2=\|f\|_{2}^2 - \|s_n\|^2+\sum_{m=1}^{n}{\left|a_m-c_m\right|^2}$. With $a_m=c_m$, this gives $\|f-s_n\|_{2}^2=\|f\|_2^{2}-\|s_n\|_{2}^2+0$.
		Hence,
		\[
			\|f-t_n\|_{2}^2=\|f-s_n\|_{2}^2+\underbrace{\sum_{m=1}^{n}{\left|a_m-c_m\right|^2}}_{\ge 0}
			,\]
		so \[
			\|f-s_n\|_{2}^2\le \|f-t_n\|_{2}^2
			,\] with equality if and only if $a_m=c_m$ for all $m=1,2,\ldots $.
	\end{proof}
	\begin{note}
		We have proven that $\|f\|_2^{2}=\|s_n\|_2^2+\|f-s_n\|_{2}^2$.
		Let $V_n$ be a subspace of linear combinations of $\{\phi_m\}_{m=1,2,\ldots ,n}$; i.e., $V_n=\left\{ \sum_{m=1}^{n}{a_m \phi_m}: a_m \in \C\right\}$.
		Then $s_n=L^2$ projection of $f$ onto $V_n$. Also, $\|s_n\|_{2}^2=\sum_{m=1}^{n}{\left|c_m\right|^2}\le \|f\|_{2}^2$.
		If we have an infinite set $\{\phi_m\}_{m \in \N}$ then we can let $n\to \infty$, and we get $\sum_{m=1}^{\infty}{\left|c_m\right|^2}\le \|f\|_2^{2}$.
		This is called \textit{Bessel's inequality}.
		In particular, $\lim_{n\to \infty}{c_n}=0$.
	\end{note}
\end{thm}

\begin{example}
	If $f \in \mathscr{R}[-\pi,\pi]$, then
	\[
		\lim_{n\to \infty}{\int_{-\pi}^{\pi}{f(x) \cos{(nx)}\mathrm{d}x}}=0
		.\]
	This is an instance of \textit{Riemann-Lebesgue lemma}.
\end{example}


From here on, we restrict to $f:[-\pi,\pi]\to \C$ with $f \in \mathscr{R}[-\pi,\pi]$ and take $\phi_n(x)=e^{inx}, n \in \Z$. We extend $f$ by $2\pi$-periodicity to $\R$; i.e., $f(x+2\pi)=f(x)$ for all $x \in \R$.\\
In order to make $\{\phi_n\}$ orthonormal, we change the definition of $<f,g>$ to
\[
	<f,g> = \frac{1}{2\pi} \int_{-\pi}^{\pi}{f(x) \overline{g(x)}\mathrm{d}x}
	,\]
and
\[
	\|f\|_2^2= <f, f> =\frac{1}{2\pi} \int_{-\pi}^{\pi}{\left|f(x)\right|^2\mathrm{d}x}
	.\]
Then, $<\phi_m \phi_n > = \delta_{mn}$, so Theorem~\ref{thm:8.11} still holds.\\


\begin{define}[Fourier Series]
	\hfill
	\begin{itemize}
		\item
		      For $f \in \mathscr{R}[-\pi,\pi]$, the \textit{Fourier series} of $f$ is
		      \[
			      f(x)=\sum_{n=-\infty}^{\infty}{c_n \phi_n(x)}
		      \] with $c_n= <f, \phi_n>$.
		      \begin{remark}
			      At this point, we don't claim that the series converges or equals $f(x)$
		      \end{remark}
		\item The \textit{$N^{\text{ th } }$ partial Fourier series} of  $f$ is \[
			      s_N(x)=\sum_{n=-N}^{N}{c_n e^{inx}}
			      .\]
		      We have seen that $\sum_{n=-N}^{N}{\left|c_{n}\right|^2}=\|s_N\|_{2}^2\le \|f\|_{2}^2$.
	\end{itemize}
\end{define}
\begin{lemma}
	For $x \in \R$, $S_N(x)=\frac{1}{2\pi} \int_{-\pi}^{\pi}{f(x-t)D_{N}(t)\mathrm{d}t}$ with $D_{N}(t)=\sum_{n=-N}^{N}{e^{int}}= \frac{\sin{(N+ \frac{1}{2})t}}{\sin{\frac{t}{2}}}$.
	\begin{remark}
		$D_N$ is called the Dirichlet kernel.
	\end{remark}
	\begin{note}
		\begin{enumerate}
			\item \[
				      \frac{1}{2\pi} \int_{-\pi}^{\pi}{D_{N}(t)\mathrm{d}t}= < D_{N}, \underbrace{1}_{\phi_0} > = \sum_{n=-N}^{N}\underbrace{<\phi_n , \phi_{0} > }_{=\delta_{n_{0}}}= 1
				      .\]
			\item \[
				      D_N(t)= \frac{1}{\sin{(\frac{t}{2})}} \left[\sin{(\frac{t}{2})} \cos{(Nt)} + \cos{(\frac{t}{2})} \sin{(Nt)}\right] =\cos{(Nt)}+ \cot{(\frac{t}{2})} \sin{(Nt)}
				      .\]
		\end{enumerate}
	\end{note}
	\begin{proof}
		\begin{description}
			\item[$D_N$]:
			      \begin{flalign*}
				      D_N(t) & =\sum_{n=-N}^{N}{e^{in t}}= e^{-iNt} \sum_{k=0}^{2N}{(e^{it})^{k}}                                                                                                                                                                                \\
				             & = e^{-iNt} \cdot \frac{e^{it(2N+1)}-1}{e^{it}-1} \cdot \frac{e^{-\frac{it}{2}}}{e^{- \frac{it}{2}}}=\frac{e^{i(N+\frac{1}{2})t}-e^{-i(N+ \frac{1}{2})t}}{e^{i \frac{t}{2}}-e^{-i \frac{t}{2}}}= \frac{\sin{(N+ \frac{1}{2})t}}{\sin{\frac{t}{2}}}
				      .\end{flalign*}
			\item [$S_N$]:
			      \begin{flalign*}
				      S_N(x) & =\sum_{n=-N}^{N}{c_n e^{inx}}= \sum_{n=-N}^{N}{<f, \phi_n > e^{inx}}                                                                                                          \\
				             & =\sum_{n=-N}^{N}{\frac{1}{2\pi} \int_{-\pi}^{\pi}{f(t) \overline{e^{int}}\mathrm{d}t} e^{inx}}=\frac{1}{2\pi} \int_{-\pi}^{\pi}{f(t) \sum_{n=-N}^{N}{e^{in(x-t)}}\mathrm{d}t} \\
				             & =\frac{1}{2\pi} \int_{-\pi}^{\pi}{f(t) D_N(x-t)\mathrm{d}t}                                                                                                                   \\
				             & =\frac{1}{2\pi} \int_{-\pi}^{\pi}{f(x-t) D_N(t)\mathrm{d}t}
				      .\end{flalign*}
			      (Last equality left as an exercise)
		\end{description}
	\end{proof}
\end{lemma}

\begin{thm}[14]
	Let $x \in \R$ and $f \in \mathscr{R}[-\pi,\pi]$ with $f(x+2\pi)=f(x)$.
	Suppose there exist $\delta,M$ such that
	\[
		\left|f(x+t)-f(x)\right|\le M \left|t\right| \text{ for } \left|t\right|\le \delta
		.\]
	Then $\lim_{N\to \infty}{S_N(x)}=f(x)$.
	\begin{remark}
		Such $f$ is said to be \textit{Lipschitz continuous} at $x$.
	\end{remark}
	\begin{proof}
		\begin{flalign*}
			f(x)-S_N(x) & = f(x) \cdot \underline{\frac{1}{2\pi} \int_{-\pi}^{\pi}{D_N(t)\mathrm{d}t}}_{=1} - \frac{1}{2\pi} \int_{-\pi}^{\pi}{f(x-t)D_N(t)\mathrm{d}t}                                                                                \\
			            & =\frac{1}{2\pi} \int_{-\pi}^{\pi}{\left[f(x)-f(x-t)\right]D_N(t) \mathrm{d}t}                                                                                                                                                \\
			            & = \underbrace{\frac{1}{2\pi} \int_{-\pi}^{\pi}{\left[f(x)-f(x-t)\right]\cos{(Nt)} \mathrm{d}t}}_{*} + \underbrace{\frac{1}{2\pi} \int_{-\pi}^{\pi}{\left[f(x)-f(x-t)\right] \cot{(\frac{t}{2})}\sin{(Nt)} \mathrm{d}t}}_{**}
			.\end{flalign*}
		Since $f(x)-f(x-t) \in \mathscr{R}(dt)$, $* \to 0$ as $N\to \infty$ by Riemann-Lebesgue lemma.\\
		For $**$, note that $\left[f(x)-f(x-t)\right] \cot{(\frac{t}{2})}= \frac{f(x)-f(x-t)}{t} \cdot 2 \frac{\frac{t}{2}}{\sin{(\frac{t}{2})}} \cos{(\frac{t}{2})}$.
		Since $\frac{f(x)-f(x-t)}{t}$ is bounded by hypothesis and $\cos{(\frac{t}{2})}$ is continuous, $** \to 0$ as $N\to \infty$ by Riemann-Lebesgue lemma.
	\end{proof}
\end{thm}
\begin{corollary}
	\begin{itemize}
		\item
		      If $f(x)=0$ for all $x \in (x_{0}-\epsilon,x_{0}+\epsilon)$ then $S_N(f;x)\to 0$ for all such $x$ because $f$ is Lipschitz continuous at $x$.
		\item If $f(x)=g(x)$ for all $x \in (x_{0}-\epsilon,x_{0}+\epsilon)$, then by linearity, $S_N(f;x)- S_N(g;x)\to 0$ as $N\to \infty$ for all such $x$.
	\end{itemize}
\end{corollary}
\begin{example}
	Suppose
	\begin{flalign*}
		f(x) & =
		\begin{cases}
			0                       & x \in (0, \pi)                         \\
			\frac{3}{\pi} x         & x \in (\pi , \frac{4}{3}\pi)           \\
			1                       & x \in (\frac{4}{3}\pi, \frac{5}{3}\pi) \\
			-\frac{3}{\pi} (x-2\pi) & x \in (\frac{5}{3}\pi, 2\pi)           \\
			f(x)=f(x+2\pi)          & \text{otherwise}
		\end{cases} \\
		g(x) & =
		\begin{cases}
			0                       & x \in (0, \pi)               \\
			\frac{2}{\pi} x         & x \in (\pi , \frac{3}{2}\pi) \\
			-\frac{2}{\pi} (x-2\pi) & x \in (\frac{3}{2}\pi, 2\pi) \\
			g(x)=g(x+2\pi)          & \text{otherwise}
		\end{cases}
		.\end{flalign*}
	Then $f,g$ have different Fourier series, but both converge to zero on $[0,\pi]$: Localisation Principle. Contract this to the behaviour of power series in Theorem~\ref{thm:8.5}.
\end{example}


\begin{thm}[15]
	If $f: \R\to \C$ is continuous and $2\pi$-periodic, then for all $\epsilon>0$, there exists a trigonometric polynomial $P(x)=\sum_{n=-N}^{N}{c_n e^{inx}}$ such that $\|f-P\|= \sup_{x \in \R} \left|f(x)-P(x)\right|<\epsilon$.
	\begin{proof}
		Let $T=\{ z \in \C \mid \left|z\right|=1\}$, the unit circle in $\C$.\\
		$T$ is compact. Define $F: T\to \C$ by $F(z)=f(t)$, where $z=e^{it}$. This is well defined by the $2\pi$-periodicity of $f$.\\
		Let $\mathscr{A}=\text{ Algebra of trigonometric polynomials } \sum_{n=-N}^{N}{a_{n}z^{n}}, \,\, z \in T, a_{n} \in \C, N=0,1,2, \ldots $. Then $\mathscr{A}$ vanishes at no point since it contains $1$.\\
		Also, $\mathscr{A}$ separates points and self-adjoint since
		\begin{flalign*}
			\sum_{n=-N}^{N}{a_{n}z^{n}} & = \sum_{n=-N}^{N}{\overline{a_n z^{n}}}      = \sum_{n=-N}^{N}{\overline{a_n}z^{-n}} \\
			                            & \underbrace{=}_{m=-n} \sum_{m=-N}^{N}{\overline{a_{-m}} z^{m}} \in \mathscr{A}
			.\end{flalign*}
		By Stone-Weierstrass, $\overline{\mathscr{A}}= \mathscr{C}(T)$, so given $\epsilon>0$, there exists $\mathscr{P} \in \mathscr{A}$ such that $\left|F(z)-\mathscr{P}(z)\right|<\epsilon$ for all $z \in T$.
		Write $\mathscr{P}(z)=\sum_{n=-N}^{N}{a_n z^{n}}$ and let $P(x)=\sum_{n=-N}^{N}{c_n e^{inx}}= \mathscr{P}(e^{ix})$.
		Then $\left|f(x)-P(x)\right|=\left|F(e^{ix})-\mathscr{P}(e^{ix})\right|<\epsilon$ for all $x \in \R$.
	\end{proof}
	\begin{remark}
		\begin{enumerate}
			\item $\mathscr{P}$ in Theorem~\ref{thm:8.15} need not involve the Fourier coefficients. In fact, there exists a continuous $f$ whose Fourier series does not converge pointwise.
			\item For a continuous $f$, Rudin's Problem 8.15 gives an explicit sequence of trigonometric polynomials that converge uniformly to $f$ (Cesaro summation of Fourier partial sums).
			\item (theorem not in Rudin's) If $f$ is $2\pi$-periodic and continuously differentiable then its Fourier series converge uniformly to $f$ (proof omitted).
			\item Theorem (Kolmogorov): There exists a Lebesgue integrable function $f$ on $[-\pi,\pi]$ whose Fourier series diverges everywhere.
		\end{enumerate}
	\end{remark}
\end{thm}

\begin{notation}
	\begin{itemize}
		\item For $\{c_{n}\}_{n \in \Z}$ and $\{\gamma_n\}_{n \in \Z}$, let $(c,\gamma) = \sum_{n=-\infty}^{\infty}{c_n \overline{\gamma}_n}$ when the series converges.
		\item $<f,g> = \frac{1}{2\pi} \int_{-\pi}^{\pi}{f(x) \overline{g(x)}\mathrm{d}x}$, $\|f\|_2^{2}= < f, f >$.
		\item $\phi_n(x)=e^{inx}, S_N(f)= \sum_{n=-N}^{N}{ <f,\phi_n> \phi_n}$.
	\end{itemize}
\end{notation}


\begin{problem}[6.10]
\label{prob:6.10}
Cauchy-Schwarz inequality: $\left|<f,g>\right|\le \|f\|_2 \|g\|_2$.
\end{problem}

\begin{problem}[6.11]
\label{prob:6.11}
Minkowski inequality: $\|f+g\|_2\le \|f\|_2+\|g\|_2$.\\
(so $\|f-g\|_2\le \|f-h\|_2 + \|h-g\|_2$)
\end{problem}

\begin{problem}[6.12]
\label{prob:6.12}
For $f \in \mathscr{R}[-\pi,\pi], \epsilon>0$, there exists a continuous function (piecewise-linear) $h$ s.t. $\|f-h\|<\epsilon$..
If $f(-\pi)=f(\pi)$ then we can choose $h$ s.t. $h(-\pi)=h(\pi)$.
\end{problem}

\begin{thm}[16]
	For $f,g \in \mathscr{R}[-\pi,\pi]$ that are $2\pi$-periodic,
	let $c_n= <f, \phi_n>, \gamma_n = < g, \phi_n>$.
	Then $\lim_{N\to \infty}{\|f-s_N(f)\|_2}=0$ (convergence of $s_N(f)$ to $f$ in $L^2$-norm), and $<f,g> = (c,\gamma)$ (Parseval's frelation).
	In particular, $\|f\|_2^{2}=(c,c)$; i.e., $\frac{1}{2\pi}\int_{-\pi}^{\pi}{\left|f(x)\right|^2\mathrm{d}x}=\sum_{n=-\infty}^{\infty}{\left|c_n\right|^2}$.
	This is called Bessel's equality.
	\begin{proof}
		\begin{description}
			\item[$L^2$ convergence:]
			      Let $\epsilon>0$. Choose a continuous $h$ (\ref{prob:6.12}) s.t. $\|f-h\|_2 < \frac{\epsilon}{3}$.
			      Then $\|s_N(f)-f\|_{2}\le \underbrace{\|s_N(f)-s_N(h)\|_2}_{(*)}+\underbrace{\|s_N(h)-h\|_{2}}_{(**)}+\underbrace{\|h-f\|_2}_{<\frac{\epsilon}{3}}$.
			      Note $(*)= \|s_N(f-h)\|_2 \le \|f-h\|_2$ from the proof of Bessel's inequality.
			      Also by Theorem~\ref{thm:8.15}, there exists a trigonometric polynomial $P$ s.t. $\|P-h\|_{\infty}<\frac{\epsilon}{3}$ and hence $\|P-h\|_2 \le  \|P-h\|_{\infty}<\frac{\epsilon}{3}$.\\
			      Say $\text{deg}(P)=N_{0}$. By Theorem~\ref{thm:8.11},
			      for $N\ge N_{0}$, \[
				      \|s_N(h)\|_{2}\le \|P-h\|_2
				      .\]
			      Thus, for $N\ge N_{0}$\[
				      \|s_N(h)-h\|_{2} < \frac{\epsilon}{3}
				      .\]
			      Therefore, $\|s_N(f)-f\|_2<\frac{\epsilon}{3}+\frac{\epsilon}{3}+\frac{\epsilon}{3}=\epsilon$ if $N\ge N_{0}$, so $\lim_{N\to \infty}{\|s_N(f)-f\|_2}=0$.
			      \begin{remark}
				      Good proof to master.
			      \end{remark}

			      \begin{note}
				      $\|f\|_2 \le  \|f\|_{\infty}$ for all $f \in \mathscr{R}[-\pi,\pi]$ since $\|f\|_2^2=\frac{1}{2\pi} \int_{-\pi}^{\pi}{\left|f(x)\right|^2\mathrm{d}x}\le \frac{1}{2\pi} \int_{-\pi}^{\pi}{\|f\|_{\infty}^2\mathrm{d}x}=\|f\|_{\infty}^2$.
			      \end{note}
			\item[Parseval relation:]\hfill \\
			      $<s_N(f),g> = \sum_{n=-N}^{N}{c_{n} \underbrace{< \phi_n , g>}_{\overline{\gamma}_n}}$.\\
			      $\left|<f ,g > - \sum_{n=-N}^{N}{c_{n}\overline{\gamma}_n}\right|= \left|<f,g> - <s_N(f),g>\right|= \left|<f-s_N(f),g>\right| \le  \|f-s_N(f)\|_2 \|g\|_2 \to 0
			      $ as $N\to \infty$, where the last inequality follows from Cauchy-Schwarz inequality (Problem~\nameref{prob:6.10}{6.10}).
			      \begin{note}
				      The symmetric sum $\sum_{n=-N}^{N}{c_{n} \overline{\gamma}_n}$ converges to $<f,g>$, but
				      \begin{flalign*}
					      \sum_{n=-N}^{N}{\left|c_{n} \overline{\gamma}_n\right|} & \le \left(\sum_{n=-N}^{N}{\left|c_n\right|^2}\right)^{\frac{1}{2}} \cdot \left(\sum_{n=-N}^{N}{\left|\overline{\gamma}_n\right|^2}\right)^{\frac{1}{2}} \\
					                                                              & \le \|f\|_2 \cdot \|g\|_2,
				      \end{flalign*}
				      so $\sum_{n=-\infty}^{\infty}{c_n \overline{\gamma}_n}$ converges absolutely and its sum is independent of how we make partial sums.
			      \end{note}
		\end{description}
	\end{proof}
\end{thm}
